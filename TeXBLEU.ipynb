{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_tex_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        corpus = file.readlines()\n",
    "    return [line.strip() for line in corpus]\n",
    "\n",
    "def process_line(line):\n",
    "    # 수식 부분을 제외하고 나머지 텍스트를 소문자로 변환\n",
    "    def lower_except_math(text):\n",
    "        if text.startswith('$') or text.startswith('\\\\[') or text.startswith('\\\\('):\n",
    "            return text\n",
    "        else:\n",
    "            return text.lower()\n",
    "\n",
    "    # 수식 부분을 제외한 텍스트를 찾기 위한 정규식\n",
    "    pattern = re.compile(r'(\\$.*?\\$|\\\\\\[.*?\\\\\\]|\\\\\\(.*?\\\\\\))')\n",
    "    parts = pattern.split(line)\n",
    "    processed_parts = [lower_except_math(part) for part in parts]\n",
    "    return ''.join(processed_parts)\n",
    "\n",
    "def process_corpus(corpus):\n",
    "    return [process_line(line) for line in corpus]\n",
    "\n",
    "# 사용 예시\n",
    "filepath = '2301.01754.tex'\n",
    "corpus = load_tex_file(filepath)\n",
    "processed_corpus = process_corpus(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TeXBLEU score for candidate1: 0.5888\n",
      "TeXBLEU score for candidate2: 0.9195\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "import math\n",
    "from collections import Counter\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "class TeXBLEU:\n",
    "    def __init__(self, corpus: List[str], vocab_size: int = 30000):\n",
    "        self.tokenizer = self._train_bpe_tokenizer(corpus, vocab_size)\n",
    "        \n",
    "    def _train_bpe_tokenizer(self, corpus: List[str], vocab_size: int) -> Tokenizer:\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "                             vocab_size=vocab_size)\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        tokenizer.train_from_iterator(corpus, trainer)\n",
    "        return tokenizer\n",
    "    \n",
    "    def preprocess_latex(self, text: str) -> str:\n",
    "        # Add space before '\\'\n",
    "        text = re.sub(r'(?<![\\\\])(\\\\)', r' \\1', text)\n",
    "        # Remove multiple spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def add_positional_encoding(self, tokens: List[str]) -> List[str]:\n",
    "        return [f\"{i}:{token}\" for i, token in enumerate(tokens)]\n",
    "    \n",
    "    def calculate_texbleu(self, reference: str, candidate: str, max_n: int = 4) -> float:\n",
    "        ref_tokens = self.tokenizer.encode(self.preprocess_latex(reference)).tokens\n",
    "        cand_tokens = self.tokenizer.encode(self.preprocess_latex(candidate)).tokens\n",
    "        \n",
    "        ref_tokens = self.add_positional_encoding(ref_tokens)\n",
    "        cand_tokens = self.add_positional_encoding(cand_tokens)\n",
    "        \n",
    "        bp = self._brevity_penalty(ref_tokens, cand_tokens)\n",
    "        \n",
    "        scores = []\n",
    "        for n in range(1, max_n + 1):\n",
    "            scores.append(self._modified_precision(ref_tokens, cand_tokens, n))\n",
    "        \n",
    "        if 0 in scores:\n",
    "            return 0\n",
    "        \n",
    "        score = bp * math.exp(sum(math.log(s) for s in scores) / max_n)\n",
    "        return score\n",
    "    \n",
    "    def _brevity_penalty(self, ref_tokens: List[str], cand_tokens: List[str]) -> float:\n",
    "        r = len(ref_tokens)\n",
    "        c = len(cand_tokens)\n",
    "        \n",
    "        if c > r:\n",
    "            return 1\n",
    "        else:\n",
    "            return math.exp(1 - r/c)\n",
    "    \n",
    "    def _modified_precision(self, ref_tokens: List[str], cand_tokens: List[str], n: int) -> float:\n",
    "        ref_ngrams = Counter(self._get_ngrams(ref_tokens, n))\n",
    "        cand_ngrams = Counter(self._get_ngrams(cand_tokens, n))\n",
    "        \n",
    "        max_counts = {}\n",
    "        for ngram, count in cand_ngrams.items():\n",
    "            max_counts[ngram] = max(0, count - max(0, count - ref_ngrams[ngram]))\n",
    "        \n",
    "        if len(cand_ngrams) == 0:\n",
    "            return 0\n",
    "        \n",
    "        return sum(max_counts.values()) / sum(cand_ngrams.values())\n",
    "    \n",
    "    def _get_ngrams(self, tokens: List[str], n: int) -> List[tuple]:\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    \"This is a sample LaTeX document.\",\n",
    "    \"\\\\begin{document} Hello, world! \\\\end{document}\",\n",
    "    \"E = mc^2\",\n",
    "    \"\\\\frac{1}{2}\",\n",
    "    \"\\\\sum_{i=1}^n i = \\\\frac{n(n+1)}{2}\"\n",
    "]\n",
    "\n",
    "texbleu = TeXBLEU(processed_corpus)\n",
    "\n",
    "reference = \"\\\\begin{equation} f(x) = \\\\int_{-\\\\infty}^\\\\infty \\\\hat f(\\\\xi)\\\\,e^{2 \\\\pi i \\\\xi x} \\\\,d\\\\xi \\\\end{equation}\"\n",
    "candidate1 = \"\\\\begin{equation} f(x) = \\\\int_{-\\\\infty}^\\\\infty \\\\hat f(\\\\xi) e^{2 \\\\pi i \\\\xi x} d\\\\xi \\\\end{equation}\"\n",
    "candidate2 = \"\\\\begin{align} f(x) = \\\\int_{-\\\\infty}^\\\\infty \\\\hat f(\\\\xi)\\\\,e^{2 \\\\pi i \\\\xi x} \\\\,d\\\\xi \\\\end{align}\"\n",
    "\n",
    "score1 = texbleu.calculate_texbleu(reference, candidate1)\n",
    "score2 = texbleu.calculate_texbleu(reference, candidate2)\n",
    "\n",
    "print(f\"TeXBLEU score for candidate1: {score1:.4f}\")\n",
    "print(f\"TeXBLEU score for candidate2: {score2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\begin{equation} f(x) = \\\\int_{-\\\\infty}^\\\\infty \\\\hat f(\\\\xi)\\\\,e^{2 \\\\pi i \\\\xi x} \\\\,d\\\\xi \\\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\begin{equation} f(x) = \\\\int_{-\\\\infty}^\\\\infty \\\\hat f(\\\\xi) e^{2 \\\\pi i \\\\xi x} d\\\\xi \\\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\begin{align} f(x) = \\\\int_{-\\\\infty}^\\\\infty \\\\hat f(\\\\xi)\\\\,e^{2 \\\\pi i \\\\xi x} \\\\,d\\\\xi \\\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
