{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def load_tex_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return [line.strip() for line in file]\n",
    "\n",
    "def process_line(line):\n",
    "    return re.sub(r'(\\$.*?\\$|\\\\\\[.*?\\\\\\]|\\\\\\(.*?\\\\\\))', '', line)\n",
    "\n",
    "def process_corpus(corpus):\n",
    "    return [process_line(line) for line in corpus]\n",
    "\n",
    "def find_tex_files(root_folder):\n",
    "    tex_files = []\n",
    "    for root, _, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.tex'):\n",
    "                tex_files.append(os.path.join(root, file))\n",
    "    return tex_files\n",
    "\n",
    "def process_all_tex_files(root_folder):\n",
    "    all_processed_corpus = []\n",
    "    tex_files = find_tex_files(root_folder)\n",
    "    \n",
    "    for filepath in tex_files:\n",
    "        corpus = load_tex_file(filepath)\n",
    "        processed_corpus = process_corpus(corpus)\n",
    "        all_processed_corpus.extend(processed_corpus)\n",
    "    \n",
    "    return all_processed_corpus\n",
    "\n",
    "# 사용 예시\n",
    "root_folder = r'C:\\Users\\wjdrb\\Downloads\\drive-download-20240719T063242Z-001'\n",
    "processed_corpus = process_all_tex_files(root_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\wjdrb\\AppData\\Local\\Temp\\ipykernel_49116\\3063883863.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  corpus = [\"F(x)={\\sqrt{\\frac{10x^{-8}}{-8}}}+C_{1} =-{\\frac{5}{4x^{8}}}+C_{1}\\quad{\\mathrm {if~}}x<0.\",\n",
      "C:\\Users\\wjdrb\\AppData\\Local\\Temp\\ipykernel_49116\\3063883863.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"F(x)=\\sqrt{10}x^{-8/-8}+C_{1}=-\\frac{5} {4}x^{8}+C_{1}\\quad\\mathrm{if~}x<0.\"]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"F(x)={\\sqrt{\\frac{10x^{-8}}{-8}}}+C_{1} =-{\\frac{5}{4x^{8}}}+C_{1}\\quad{\\mathrm {if~}}x<0.\",\n",
    "\"F(x)=\\sqrt{10}x^{-8/-8}+C_{1}=-\\frac{5} {4}x^{8}+C_{1}\\quad\\mathrm{if~}x<0.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer, trainers, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from typing import List\n",
    "\n",
    "def train_and_upload_tokenizer(corpus: List[str], vocab_size: int = 30000, repo_name: str = \"Kyudan/TeXBLUE-Tokenizer\"):\n",
    "    # 토크나이저와 트레이너 초기화\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "                         vocab_size=vocab_size)\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # tqdm을 사용하여 진행 상태 표시\n",
    "    print(\"Training tokenizer...\")\n",
    "    corpus_with_progress = tqdm(corpus, desc=\"Training Progress\", total=len(corpus))\n",
    "    tokenizer.train_from_iterator(corpus_with_progress, trainer)\n",
    "\n",
    "    # 토크나이저를 파일로 저장\n",
    "    tokenizer.save(\"tokenizer.json\")\n",
    "    \n",
    "    # Hugging Face Hub에 업로드\n",
    "    api = HfApi()\n",
    "    token = HfFolder.get_token()\n",
    "    \n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"tokenizer.json\",\n",
    "        path_in_repo=\"tokenizer.json\",\n",
    "        repo_id=repo_name,\n",
    "        token=token,\n",
    "    )\n",
    "    \n",
    "    print(f\"Tokenizer uploaded to Hugging Face Hub under the repository {repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 119194221/119194221 [02:42<00:00, 732722.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer uploaded to Hugging Face Hub under the repository Kyudan/TeXBLUE-Tokenizer\n"
     ]
    }
   ],
   "source": [
    "train_and_upload_tokenizer(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "import math\n",
    "from collections import Counter\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import Tokenizer\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "def load_tokenizer_from_hub(repo_name: str = \"Kyudan/TeXBLEU-Tokenizer\") -> Tokenizer:\n",
    "    # Hugging Face Hub에서 토크나이저 다운로드\n",
    "        snapshot_dir = snapshot_download(repo_id=repo_name)\n",
    "    \n",
    "    # 다운로드한 토크나이저 불러오기\n",
    "        tokenizer = Tokenizer.from_file(f\"{snapshot_dir}/tokenizer.json\")\n",
    "        return tokenizer\n",
    "\n",
    "\n",
    "class TeXBLEU:\n",
    "    # List: 문자열들의 리스트.\n",
    "    def __init__(self, repo_name: str):\n",
    "        self.tokenizer = self._load_tokenizer_from_hub(repo_name)\n",
    "    \n",
    "    def _load_tokenizer_from_hub(self, repo_name: str) -> Tokenizer:\n",
    "        return load_tokenizer_from_hub(repo_name)\n",
    "\n",
    "    #스트링 형으로 이루어진 리스트와 int형 vocab size를 입력으로 받아서  Tokenizer 객체를 반환한다.\n",
    "    def _train_bpe_tokenizer(self, corpus: List[str], vocab_size: int) -> Tokenizer:\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "                             vocab_size=vocab_size)\n",
    "        # tokenizer의 전처리 단계로 공백을 기준으로 분리하는 Whitespace() 전처리기를 설정.\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        tokenizer.train_from_iterator(corpus, trainer)\n",
    "        return tokenizer\n",
    "    \n",
    "    # 명령어 앞에 한칸 띄고, 여러 공백은 지우기.\n",
    "    def preprocess_latex(self, text: str) -> str:\n",
    "        # Add space before '\\'\n",
    "        text = re.sub(r'(?<![\\\\])(\\\\)', r' \\1', text)\n",
    "        # Remove multiple spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    \"\"\"\n",
    "    input:   tokens = [\"\\\\frac\", \"{\", \"1\", \"}\", \"{\", \"2\", \"}\"]\n",
    "\n",
    "    output:           [\"0:\\\\frac\", \"1:{\", \"2:1\", \"3:}\", \"4:{\", \"5:2\", \"6:}\"]\n",
    "\n",
    "    \"\"\"\n",
    "    def add_positional_encoding(self, tokens: List[str]) -> List[str]:\n",
    "        return [f\"{i}:{token}\" for i, token in enumerate(tokens)]\n",
    "    \n",
    "    def calculate_texbleu(self, reference: str, candidate: str, max_n: int = 4) -> float:\n",
    "        ref_tokens = self.tokenizer.encode(self.preprocess_latex(reference)).tokens\n",
    "        cand_tokens = self.tokenizer.encode(self.preprocess_latex(candidate)).tokens\n",
    "        print(f\"ref_tokens : {ref_tokens}\")\n",
    "        print(f\"cand_tokens : {cand_tokens}\")\n",
    "        # 아래 두줄을 주석으로 두면 positional encoding 끄게됨.\n",
    "        ref_tokens = self.add_positional_encoding(ref_tokens)\n",
    "        cand_tokens = self.add_positional_encoding(cand_tokens)\n",
    "        \n",
    "        bp = self._brevity_penalty(ref_tokens, cand_tokens)\n",
    "        \n",
    "        scores = []\n",
    "        for n in range(1, max_n + 1):\n",
    "            scores.append(self._modified_precision(ref_tokens, cand_tokens, n)) # n은 n-gram의 n임.\n",
    "        \n",
    "        if 0 in scores:\n",
    "            return 0\n",
    "        \n",
    "        score = bp * math.exp(sum(math.log(s) for s in scores) / max_n) #modified_precision들의 평균을 exp취한 것에 bp 를 곱함.\n",
    "        return score\n",
    "    \n",
    "    def _brevity_penalty(self, ref_tokens: List[str], cand_tokens: List[str]) -> float:\n",
    "        r = len(ref_tokens)\n",
    "        c = len(cand_tokens)\n",
    "        \n",
    "        if c > r:\n",
    "            return 1\n",
    "        else:\n",
    "            return math.exp(1 - r/c)\n",
    "    \n",
    "    def _modified_precision(self, ref_tokens: List[str], cand_tokens: List[str], n: int) -> float:\n",
    "        ref_ngrams = Counter(self._get_ngrams(ref_tokens, n))\n",
    "        cand_ngrams = Counter(self._get_ngrams(cand_tokens, n))\n",
    "        \n",
    "        max_counts = {}\n",
    "        for ngram, count in cand_ngrams.items():\n",
    "            max_counts[ngram] = max(0, count - max(0, count - ref_ngrams[ngram]))\n",
    "        \n",
    "        if len(cand_ngrams) == 0:\n",
    "            return 0\n",
    "        \n",
    "        return sum(max_counts.values()) / sum(cand_ngrams.values())\n",
    "    \n",
    "    \"\"\"\n",
    "    예시:\n",
    "    입력: tokens = [\"The\", \"quick\", \"brown\", \"fox\"], n = 2\n",
    "    출력: [(\"The\", \"quick\"), (\"quick\", \"brown\"), (\"brown\", \"fox\")]\n",
    "    \"\"\"\n",
    "    def _get_ngrams(self, tokens: List[str], n: int) -> List[tuple]:\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F(x)=\\sqrt{10}x^{-8/-8}+C_{1}=-\\frac{5} {4}x^{8}+C_{1}\\quad\\mathrm{if~}x<0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F(x)=\\sqrt{10}x^{-8/-8}+C_{1}=-\\frac{5} {4}x^{8}+C_{1}\\quad\\mathrm{if~}x<0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F(x)={\\sqrt{\\frac{10x^{-8}}{-8}}}+C_{1} =-{\\frac{5}{4x^{8}}}+C_{1}\\quad{\\mathrm {if~}}x<0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F(x)={\\sqrt{\\frac{10x^{-8}}{-8}}}+C_{1} =-{\\frac{5}{4x^{8}}}+C_{1}\\quad{\\mathrm {if~}}x<0.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec389bfff6b64e099e8e8d901e0f1fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref_tokens : ['F', '(', 'x', ')=', '{', '\\\\', 'sqrt', '{', '\\\\', 'frac', '{', '10', 'x', '^{-', '8', '}}', '{-', '8', '}}}', '+', 'C_', '{', '1', '}', '=-', '{', '\\\\', 'frac', '{', '5', '}{', '4x', '^{', '8', '}}}', '+', 'C_', '{', '1', '}', '\\\\', 'quad', '{', '\\\\', 'mathrm', '{', 'if', '~', '}}', 'x', '<', '0', '.']\n",
      "cand_tokens : ['F', '(', 'x', ')=', '\\\\', 'sqrt', '{', '10', '}', 'x', '^{-', '8', '/-', '8', '}+', 'C_', '{', '1', '}=-', '\\\\', 'frac', '{', '5', '}', '{', '4', '}', 'x', '^{', '8', '}+', 'C_', '{', '1', '}', '\\\\', 'quad', '\\\\', 'mathrm', '{', 'if', '~}', 'x', '<', '0', '.']\n",
      "TeXBLEU score for candidate1: 0.0473\n"
     ]
    }
   ],
   "source": [
    "texbleu = TeXBLEU(\"Kyudan/TeXBLEU-Tokenizer\")\n",
    "\n",
    "\n",
    "reference = r\"F(x)={\\sqrt{\\frac{10x^{-8}}{-8}}}+C_{1} =-{\\frac{5}{4x^{8}}}+C_{1}\\quad{\\mathrm {if~}}x<0.\"\n",
    "candidate1 = r\"F(x)=\\sqrt{10}x^{-8/-8}+C_{1}=-\\frac{5} {4}x^{8}+C_{1}\\quad\\mathrm{if~}x<0.\"\n",
    "\n",
    "score1 = texbleu.calculate_texbleu(reference, candidate1)\n",
    "print(f\"TeXBLEU score for candidate1: {score1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
